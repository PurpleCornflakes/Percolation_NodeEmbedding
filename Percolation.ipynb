{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count $<i,j>$ pairs as inputs to learn Node Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from random import choices\n",
    "import copy\n",
    "from scipy.sparse import issparse\n",
    "from statistics import mean,stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find percolation threshold. Avarage size of largest cluster $<s_{max}>$ v.s. $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Transform edgelist to Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matfile(file_, variable_name=\"network\", unDirected=True, unWeighted = True):\n",
    "    mat_variables = loadmat(file_)\n",
    "    mat_matrix = mat_variables[variable_name]\n",
    "    if issparse(mat_matrix):\n",
    "        if unDirected:\n",
    "            G = nx.Graph()\n",
    "        else:\n",
    "            G = nx.DiGraph()\n",
    "        '''\n",
    "        from_nodes: cx.row\n",
    "        to_nodes: cx.col\n",
    "        link weights: cx.data\n",
    "        '''\n",
    "        cx = mat_matrix.tocoo()\n",
    "        if unWeighted:\n",
    "            edge_list = np.array([cx.row, cx.col]).T\n",
    "            G.add_edges_from(edge_list)\n",
    "        else:\n",
    "            edge_list = np.array([cx.row, cx.col, cx.data]).T\n",
    "            G.add_weighted_edges_from(edge_list)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percolate(G, beta, unWeighted=True):\n",
    "    '''\n",
    "    Here we don't consider Directed/Weighted graph \n",
    "    \n",
    "    '''\n",
    "    edges_percolated = []\n",
    "    G_percolated = nx.Graph()\n",
    "    assert(G_percolated.number_of_nodes() == 0)\n",
    "    numPercolatedEdges = int(beta*G.number_of_edges())\n",
    "    G_percolated.add_nodes_from(list(G.nodes))\n",
    "    if unWeighted:\n",
    "        edges_percolated = choices(list(G.edges), k=numPercolatedEdges)\n",
    "        assert(len(edges_percolated) ==  numPercolatedEdges)\n",
    "        G_percolated.add_edges_from(edges_percolated)\n",
    "#     else:\n",
    "#         bond percolation probability depends on the edge's weight\n",
    "    return G_percolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clusters(G, beta, numIter, threshold):\n",
    "    Gc_sizes = []\n",
    "    Clusters_sizes = []\n",
    "    for i in range(numIter):\n",
    "        G_percolated = percolate(G, beta)\n",
    "        clusters = []\n",
    "        # nx.connected_component_subgraphs is a iterator\n",
    "        for c in list(nx.connected_component_subgraphs(G_percolated)):\n",
    "            if c.number_of_nodes() > threshold:\n",
    "                size = c.number_of_nodes()\n",
    "                clusters.append(size)     \n",
    "                Clusters_sizes.append(size)\n",
    "                \n",
    "        Gc_sizes.append(max(clusters))\n",
    "        \n",
    "    return Gc_sizes, Clusters_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ClustersStatistics(G, betas, numIter, threshold, logging):\n",
    "    GcSizes_betas = []\n",
    "    ClustersSizes_betas = []\n",
    "    for beta in betas:\n",
    "        Gc_sizes, Clusters_sizes = count_clusters(G = G, \n",
    "                                                  beta = beta,\n",
    "                                                  numIter = numIter,\n",
    "                                                  threshold=threshold)\n",
    "        GcSizes_betas.append(Gc_sizes)\n",
    "        ClustersSizes_betas.append(Clusters_sizes)\n",
    "        if logging:\n",
    "            print(\"beta = {0:.4f} calculated\".format(beta))\n",
    "    return GcSizes_betas,ClustersSizes_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_averGcSizes(GcSizes_betas, betas):\n",
    "    plt.figure(1)\n",
    "    Gc_data = np.array([[mean(sizes),stdev(sizes)] for sizes in GcSizes_betas])\n",
    "    # y is meanOfSizes, yerr is stdevOfSizes\n",
    "    plt.errorbar(x = betas, y = Gc_data[:,0], yerr = Gc_data[:,1], fmt='o')\n",
    "    plt.xlabel('beta')\n",
    "    plt.ylabel('Gc_size')\n",
    "\n",
    "def plot_GcSizes(GcSizes_betas, betas):\n",
    "    plt.figure(2)\n",
    "    for i,beta in enumerate(betas):\n",
    "        plt.plot([beta]*len(GcSizes_betas[i]),GcSizes_betas[i], ',')\n",
    "\n",
    "#             plt.scatter([beta]*len(GcSizes_betas[i]),GcSizes_betas[i], s = 1,facecolors = 'none', edgecolors='r')\n",
    "    plt.xlabel('beta')\n",
    "    plt.ylabel('Gc_size')\n",
    "    plt.xlim(min(betas)-0.0001,max(betas)+0.0001)\n",
    "def plot_averClustersSizes(ClustersSizes_betas, betas):\n",
    "    plt.figure(3)\n",
    "    CS_data = np.array([[mean(sizes),stdev(sizes)] for sizes in ClustersSizes_betas])\n",
    "    # y is meanOfSizes, yerr is stdevOfSizes\n",
    "    plt.errorbar(x = betas, y = CS_data[:,0], yerr = CS_data[:,1], fmt='o')\n",
    "\n",
    "def plot_ClustersSizes(ClustersSizes_betas, betas):\n",
    "    plt.figure(4)\n",
    "    for i,beta in enumerate(betas):\n",
    "            plt.plot([beta]*len(ClustersSizes_betas[i]),ClustersSizes_betas[i],',')\n",
    "#         plt.scatter([beta]*len(ClustersSizes_betas[i]),ClustersSizes_betas[i],s = 10, facecolors = 'none', edgecolors='r')\n",
    "    plt.xlim(min(betas)-0.0001,max(betas)+0.0001)\n",
    "#     plt.ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_numClusters(G, betas, thresholds, numIter):\n",
    "    aver_numClusters = np.zeros([len(betas),len(thresholds)])\n",
    "    for n in range(numIter):\n",
    "        numClusters = []\n",
    "        for beta in betas:  \n",
    "            G_percolated = percolate(G, beta)\n",
    "            Clusters_sizes = []\n",
    "            numClusters_t = []\n",
    "            # nx.connected_component_subgraphs is a iterator\n",
    "            for t in thresholds:\n",
    "                clusters_sizes = []\n",
    "                for c in list(nx.connected_component_subgraphs(G_percolated)):\n",
    "                    size = c.number_of_nodes()\n",
    "                    if size > t:\n",
    "                        clusters_sizes.append(size)     \n",
    "                numClusters_t.append((sum(clusters_sizes)-max(clusters_sizes)))\n",
    "            numClusters.append(numClusters_t)\n",
    "        print(\"num_iter = \", n)\n",
    "        aver_numClusters += numClusters\n",
    "    return aver_numClusters/numIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numClusters(numClusters):\n",
    "    data_numClusters = np.array(numClusters).T\n",
    "    for i,t in enumerate(thresholds):\n",
    "        plt.figure(i+5)\n",
    "        plt.plot(betas, data_numClusters[i,:])\n",
    "    #         plt.scatter([beta]*len(ClustersSizes_betas[i]),ClustersSizes_betas[i],s = 10, facecolors = 'none', edgecolors='r')\n",
    "        plt.xlim(min(betas)-0.0001,max(betas)+0.0001)\n",
    "        plt.ylim(0,1300)\n",
    "        plt.title(\"threshold = \"+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # \"blogcatalog.mat\" is a undirected, unweighted graph\n",
    "    G_blogcatalog = load_matfile(file_ = \"blogcatalog.mat\")\n",
    "    assert(G_blogcatalog.number_of_nodes() == 10312)\n",
    "    assert(G_blogcatalog.number_of_edges() == 333983)\n",
    "    \n",
    "    # percolate\n",
    "    betas = np.linspace(0.002,0.01,21)\n",
    "    GcSizes_betas,ClustersSizes_betas = do_ClustersStatistics(G = G_blogcatalog, \n",
    "                                                              betas = betas, \n",
    "                                                              numIter = 20, \n",
    "                                                              threshold = 10, \n",
    "                                                              logging=False)\n",
    "    plot_averGcSizes(GcSizes_betas, betas)\n",
    "    plot_GcSizes(GcSizes_betas, betas)\n",
    "    plot_averClustersSizes(ClustersSizes_betas, betas)\n",
    "    plot_ClustersSizes=(ClustersSizes_betas, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = cal_numClusters(G=G_blogcatalog, \n",
    "                              betas = np.linspace(0.003,0.008, 21), \n",
    "                              thresholds = [1, 10, 20], \n",
    "                              numIter = 2)\n",
    "plot_numClusters(numClusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
